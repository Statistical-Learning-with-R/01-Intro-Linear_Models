---
title: "Linear Models Review"
author: "YOUR NAME HERE"
format: 
  html:
    code-fold: true
    code-line-numbers: true
    code-tools: true
editor: visual
jupyter: python3
---

## Setup

Declare your libraries:


```{python}
#| label: libraries-py
#| include: false
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
```

# The Dataset

Today we will use a dataset containing counts of babies born in California with the name "Kelly" between 1989 and 2016. Run the chunk below to load the data, and to check out the first few rows of your dataset.


```{python}
kelly_df = pd.read_csv("https://www.dropbox.com/s/jbqpw6yevaugtth/kellys_ca.csv?dl=1")
kelly_df.head()
```

The first thing we need to do with any dataset is check for missing data, and make sure the variables are the right type:


```{python}
kelly_df.info()
```

Looks good for this data, with one exception:



The "Gender" variable is being treated as an "object" variable, i.e., just an ordinary word.

But we know it should be *categorical*, aka a *factor* variable. Let's retype the variable to fix this:


```{python}
kelly_df['Gender'] = kelly_df['Gender'].astype('category')
kelly_df.info()
```

The next thing we should do is visualize our variables to get a feel for what is going on in this data.


```{python}
plt.plot(kelly_df['Year'], kelly_df['Count'])
plt.show()
```

Hmmm.... clearly there is some kind of linear-ish decreasing trend in number of Kellys. But what's going on with the two separate lines?


## Fitting a model

Let's begin with a simple linear model. We will focus only on the female-assigned Kelly babies for now.


```{python}
kellys_fem = kelly_df[kelly_df['Gender'] == "F"]
```

Our first step is to establish a which model(s) we want to try on the data.

For now, this is just a simple linear model.



To establish the model, we use the `LinearRegression` function was imported from the *scikit-learn* package in our setup chunk. 


```{python}
lin_reg = LinearRegression()

```

Next, we will **fit** the model to our data:


```{python}
lin_reg.fit(X = kellys_fem[['Year']], y = kellys_fem['Count'])
```

Let's check out the output of this model fit:


```{python}
lin_reg.coef_
r2_score(kellys_fem['Count'], lin_reg.predict(kellys_fem[["Year"]]))
```

How do we interpret this?

-   The slope is -29.97. That means there are about 30 fewer female babies named Kelly born each year since 1989!


-   The r-squared value is 0.9605. This means 96.05% of the variance in Kelly names over the years is explained by the name going out of style as time passes.

## Residuals

Now let's look the residuals of the model.

First, we can find out what values were predicted by the model:


```{python}
preds = lin_reg.predict(kellys_fem[['Year']])
```

Then, we can calculate and visualize the residuals:


```{python}
resids = preds - kellys_fem['Count']

plt.plot(kellys_fem['Year'], resids)
plt.show()
```

Do the residuals seem to represent "random noise"?

That is, was our choice of model reasonable?

## Metrics

If we are trying to find the "best" model, we should measure how well this one did.

We can compute the SSE and MSE "by hand":


```{python}
sum(resids**2)
sum(resids**2)/28
```

# YOUR TURN

What about Gender? Is the pattern of decreasing Kelly babies the same for the two Genders? Can you include Gender in the model, and interpret the results?

With your group, create a new notebook (you can copy this one if you want), and fit a model using both Year *and* Gender as predictors. Report your results.
